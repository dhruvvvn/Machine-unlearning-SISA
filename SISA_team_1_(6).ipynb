{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jmTiAoy2gUk",
        "outputId": "4685e116-4e15-466c-ccf2-5d4328b8ec33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Directory 'sisa_data/' is ready.\n",
            " Running data preprocessing...\n",
            " Dataset loaded successfully\n",
            "No missing values found\n",
            " Existing user ID column ('user_id') found\n",
            "  - Preprocessing complete\n",
            "\n",
            " Running SISA splitting and mapping\n",
            "SISA logic with detailed mapping completed successfully.\n",
            "\n",
            " Saving all output files...\n",
            "\n",
            "==================================================\n",
            " TEAM 1 EXECUTION COMPLETE \n",
            "==================================================\n",
            "  12 split files have been saved in the 'sisa_data/' folder.\n",
            "  User mapping for 20 users saved to 'sisa_data/user_mapping.json'.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        }
      ],
      "source": [
        "# =======================================================\n",
        "# TEAM 1 - COMPLETE DATA PREPARATION PIPELINE (CORRECTED VERSION)\n",
        "#\n",
        "# This script consolidates all the work from Team 1.\n",
        "# It preprocesses the data, runs the SISA splitting logic,\n",
        "# saves all output files, and runs validation checks.\n",
        "\n",
        "\n",
        "# To run:\n",
        "# 1. Make sure 'diabetes_with_users_reordered.csv' is in the same directory.\n",
        "# 2. Run the script from your terminal\n",
        "# To change the dataset, uplaod it in the directory, and change the INPUT_DATASET in CELL 3\n",
        "# =======================================================\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# =======================================================\n",
        "# CELL 1: DATA PREPROCESSOR\n",
        "# =======================================================\n",
        "def _detect_user_id_column(dataframe: pd.DataFrame) -> str:\n",
        "    \"\"\"Helper function to find a user ID column.\"\"\"\n",
        "    possible_names = ['user_id', 'userid', 'user', 'uid', 'id']\n",
        "    lower_cols = {col.lower(): col for col in dataframe.columns}\n",
        "    for name in possible_names:\n",
        "        if name in lower_cols:\n",
        "            return lower_cols[name]\n",
        "    # Raise error only if no column is found, to be caught below\n",
        "    raise ValueError(\"No user identifier column found.\")\n",
        "\n",
        "def prepare_dataset(filepath):\n",
        "    #Loads, cleans, and prepares the dataset.\n",
        "    print(\" Running data preprocessing...\")\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(\" Dataset loaded successfully\")\n",
        "\n",
        "    if df.isnull().values.any():\n",
        "        df = df.fillna(df.mean(numeric_only=True))\n",
        "    else:\n",
        "        print(\"No missing values found\")\n",
        "\n",
        "    non_numeric_cols = df.select_dtypes(exclude=['number']).columns\n",
        "    if len(non_numeric_cols) > 0:\n",
        "        encoder = LabelEncoder()\n",
        "        for col in non_numeric_cols:\n",
        "            df[col] = encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "    # Try to find a user_id column. If not found, create one.\n",
        "    try:\n",
        "        user_id_col = _detect_user_id_column(df)\n",
        "        print(f\" Existing user ID column ('{user_id_col}') found\")\n",
        "    except ValueError:\n",
        "        print(\" No user ID column found. Creating a new 'user_id' column\")\n",
        "        df[\"user_id\"] = np.arange(1, len(df) + 1)\n",
        "\n",
        "    df[\"index\"] = df.index\n",
        "    print(\"  - Preprocessing complete\")\n",
        "    return df\n",
        "\n",
        "# =======================================================\n",
        "# CELL 2: SISA LOGIC\n",
        "# =======================================================\n",
        "def create_splits_and_mapping(dataframe, num_shards, splits_per_shard):\n",
        "    \"\"\"Creates shards, splits, and a DETAILED mapping of users to their data locations.\"\"\"\n",
        "    print(\"\\n Running SISA splitting and mapping\")\n",
        "    user_id_col = _detect_user_id_column(dataframe)\n",
        "    shuffled_df = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    shards = np.array_split(shuffled_df, num_shards)\n",
        "\n",
        "    all_splits = []\n",
        "    user_mapping = {}\n",
        "\n",
        "    for shard_idx, shard_df in enumerate(shards):\n",
        "        if shard_df.empty: continue\n",
        "        splits = np.array_split(shard_df, splits_per_shard)\n",
        "\n",
        "        for split_idx, split_df in enumerate(splits):\n",
        "            if split_df.empty: continue\n",
        "            all_splits.append(split_df)\n",
        "\n",
        "            for _, row in split_df.iterrows():\n",
        "                user_id = int(row[user_id_col])\n",
        "                original_index = int(row['index'])\n",
        "\n",
        "                if user_id not in user_mapping:\n",
        "                    user_mapping[user_id] = {\n",
        "                        'original_rows': [],\n",
        "                        'locations': {}\n",
        "                    }\n",
        "\n",
        "                user_mapping[user_id]['original_rows'].append(original_index)\n",
        "\n",
        "                location_key = (shard_idx, split_idx)\n",
        "                if location_key not in user_mapping[user_id]['locations']:\n",
        "                    user_mapping[user_id]['locations'][location_key] = {\n",
        "                        'shard': shard_idx,\n",
        "                        'split': split_idx,\n",
        "                        'rows': []\n",
        "                    }\n",
        "\n",
        "                user_mapping[user_id]['locations'][location_key]['rows'].append(original_index)\n",
        "\n",
        "    #locations dict to the required list format\n",
        "    for user_id in user_mapping:\n",
        "        user_mapping[user_id]['locations'] = list(user_mapping[user_id]['locations'].values())\n",
        "        # Also sort the original_rows list for consistency\n",
        "        user_mapping[user_id]['original_rows'].sort()\n",
        "\n",
        "    print(\"SISA logic with detailed mapping completed successfully.\")\n",
        "    return all_splits, user_mapping\n",
        "\n",
        "# =======================================================\n",
        "# CELL 3 & 4: MAIN EXECUTION AND VALIDATION\n",
        "# =======================================================\n",
        "def main():\n",
        "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
        "    # --- Configuration\n",
        "    INPUT_DATASET = 'diabetes_with_users_reordered.csv'\n",
        "    OUTPUT_DIR = 'sisa_data/'\n",
        "    NUM_SHARDS = 4\n",
        "    SPLITS_PER_SHARD = 3\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    print(f\" Directory '{OUTPUT_DIR}' is ready.\")\n",
        "\n",
        "    prepared_df = prepare_dataset(INPUT_DATASET)\n",
        "    all_splits, user_map = create_splits_and_mapping(prepared_df, NUM_SHARDS, SPLITS_PER_SHARD)\n",
        "\n",
        "    print(\"\\n Saving all output files...\")\n",
        "    split_counter = 0\n",
        "    for shard_idx in range(NUM_SHARDS):\n",
        "        for split_idx in range(SPLITS_PER_SHARD):\n",
        "            if split_counter < len(all_splits):\n",
        "                split_df = all_splits[split_counter]\n",
        "                file_name = f'shard_{shard_idx}_split_{split_idx}.csv'\n",
        "                file_path = os.path.join(OUTPUT_DIR, file_name)\n",
        "                split_df.to_csv(file_path, index=False)\n",
        "                split_counter += 1\n",
        "\n",
        "    mapping_file_path = os.path.join(OUTPUT_DIR, 'user_mapping.json')\n",
        "    with open(mapping_file_path, 'w') as f:\n",
        "        json.dump(user_map, f, indent=4)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" TEAM 1 EXECUTION COMPLETE \")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"  {split_counter} split files have been saved in the '{OUTPUT_DIR}' folder.\")\n",
        "    print(f\"  User mapping for {len(user_map)} users saved to '{mapping_file_path}'.\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTJJ9aHSpQ8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/sisa_data.zip /content/sisa_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsKZ0CX0knFq",
        "outputId": "be7ae05f-709c-448d-85b9-8883a6960420"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/sisa_data/ (stored 0%)\n",
            "updating: content/sisa_data/user_mapping.json (deflated 91%)\n",
            "updating: content/sisa_data/shard_0_split_0.csv (deflated 52%)\n",
            "updating: content/sisa_data/shard_3_split_0.csv (deflated 53%)\n",
            "updating: content/sisa_data/shard_2_split_1.csv (deflated 53%)\n",
            "updating: content/sisa_data/shard_2_split_2.csv (deflated 53%)\n",
            "updating: content/sisa_data/shard_0_split_1.csv (deflated 51%)\n",
            "updating: content/sisa_data/shard_3_split_1.csv (deflated 52%)\n",
            "updating: content/sisa_data/shard_1_split_0.csv (deflated 52%)\n",
            "updating: content/sisa_data/shard_3_split_2.csv (deflated 52%)\n",
            "updating: content/sisa_data/shard_2_split_0.csv (deflated 52%)\n",
            "updating: content/sisa_data/shard_1_split_2.csv (deflated 52%)\n",
            "updating: content/sisa_data/shard_0_split_2.csv (deflated 53%)\n",
            "updating: content/sisa_data/shard_1_split_1.csv (deflated 52%)\n"
          ]
        }
      ]
    }
  ]
}